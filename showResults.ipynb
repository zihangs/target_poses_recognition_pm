{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_precision(stringList, answer):\n",
    "    goal_count = 0\n",
    "    found = 0\n",
    "    for result in stringList:\n",
    "        if result and int(result) == int(answer):\n",
    "            found = 1\n",
    "        goal_count += 1\n",
    "\n",
    "    return found/(goal_count-1)\n",
    "\n",
    "def func_recall(stringList, answer):\n",
    "    found = 0\n",
    "    for result in stringList:\n",
    "        if result and int(result) == int(answer):\n",
    "            found = 1\n",
    "            break\n",
    "    return found\n",
    "\n",
    "# def func_f1(total, stringList, answer):\n",
    "#     tp = 0\n",
    "#     tn = 0\n",
    "#     fp = 0\n",
    "#     fn = 0\n",
    "#     for result in stringList[0:-1]:\n",
    "#         if result and int(result) == int(answer):\n",
    "#             tp += 1\n",
    "#         else:\n",
    "#             fp += 1\n",
    "    \n",
    "#     fn = 1 - tp\n",
    "    \n",
    "#     # total is the number of all goals\n",
    "#     tn = total - tp - fp - fn\n",
    "#     return 2*tp/(2*tp + fp + fn)\n",
    "\n",
    "def func_accuracy(total, stringList, answer):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for result in stringList[0:-1]:\n",
    "        if result == str(answer):\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    \n",
    "    fn = 1 - tp\n",
    "    \n",
    "    # total is the number of all goals\n",
    "    tn = total - tp - fp - fn\n",
    "    return (tp + tn)/(tn + tp + fp + fn)\n",
    "\n",
    "def func_accuracy(total, stringList, answer):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for result in stringList[0:-1]:\n",
    "        if result and int(result) == int(answer):\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    \n",
    "    fn = 1 - tp\n",
    "    \n",
    "    # total is the number of all goals\n",
    "    tn = total - tp - fp - fn\n",
    "    return (tp + tn)/(tn + tp + fp + fn)\n",
    "\n",
    "\n",
    "def func_bacc(total, stringList, answer):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for result in stringList[0:-1]:\n",
    "        if result and int(result) == int(answer):\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    \n",
    "    fn = 1 - tp\n",
    "    \n",
    "    # total is the number of all goals\n",
    "    tn = total - tp - fp - fn\n",
    "\n",
    "    tpr = tp/(tp + fn)\n",
    "    tnr = tn/(tn + fp)\n",
    "    bacc = (tpr + tnr)/2\n",
    "\n",
    "    return bacc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# return a list of each statistics for every testing case\n",
    "def calculate_statistics(rows):\n",
    "    length = rows.shape[0]\n",
    "\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1_score = []\n",
    "    accuracy = []\n",
    "    b_accuracy = []\n",
    "        \n",
    "    for index, row in rows.iterrows():\n",
    "        \n",
    "        answer = row[\"Real_Goal\"]\n",
    "        results = row[\"Results\"].split(\"/\")\n",
    "        all_candidates = row[\"Cost\"].split(\"/\")\n",
    "        \n",
    "        total = len(all_candidates)-1   # the last one is /\n",
    "        \n",
    "        p = func_precision(results, answer)\n",
    "        r = func_recall(results, answer)\n",
    "        f = func_f1(total, results, answer)\n",
    "        a = func_accuracy(total, results, answer)\n",
    "        bacc = func_bacc(total, results, answer)\n",
    "        \n",
    "        precision.append(p)\n",
    "        recall.append(r)\n",
    "        f1_score.append(f)\n",
    "        accuracy.append(a)\n",
    "        b_accuracy.append(bacc)\n",
    "    \n",
    "    return precision, recall, f1_score, accuracy, b_accuracy\n",
    "\n",
    "\n",
    "def calculate_statistics_ensemble(rows1, rows2):\n",
    "    length = rows1.shape[0]\n",
    "\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1_score = []\n",
    "    accuracy = []\n",
    "    b_accuracy = []\n",
    "        \n",
    "    for index, _ in rows1.iterrows():\n",
    "        answer = rows1.at[index, \"Real_Goal\"]\n",
    "        results1 = rows1.at[index, \"Results\"].split(\"/\")\n",
    "        results2 = rows2.at[index, \"Results\"].split(\"/\")\n",
    "        intersection = list(set(results1).intersection(results2))\n",
    "        if len(intersection) == 1:   ## only have \"\", empty\n",
    "            intersection = results1  ## let rows1 always be PM\n",
    "        all_candidates = rows1.at[index, \"Cost\"].split(\"/\")\n",
    "        results = intersection\n",
    "        results.sort(reverse=True)\n",
    "        total = len(all_candidates)-1   # the last one is /\n",
    "        \n",
    "        p = func_precision(results, answer)\n",
    "        r = func_recall(results, answer)\n",
    "        f = func_f1(total, results, answer)\n",
    "        a = func_accuracy(total, results, answer)\n",
    "        bacc = func_bacc(total, results, answer)\n",
    "        precision.append(p)\n",
    "        recall.append(r)\n",
    "        f1_score.append(f)\n",
    "        accuracy.append(a)\n",
    "        b_accuracy.append(bacc)\n",
    "    \n",
    "    return precision, recall, f1_score, accuracy, b_accuracy\n",
    "\n",
    "sid = 1\n",
    "df1 = pd.read_csv(\"pm/f1_def_results_%s.csv\" % sid)\n",
    "df2 = pd.read_csv(\"lda/dynamic_lda_sub_%s.csv\" % sid)\n",
    "p,r,f1,a,bacc = calculate_statistics_ensemble(df1, df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4111111111111111\n",
      "0.37777777777777777\n",
      "0.45555555555555555\n",
      "0.6111111111111112\n",
      "0.28888888888888886\n",
      "0.3333333333333333\n",
      "0.24444444444444444\n",
      "0.3111111111111111\n",
      "0.32222222222222224\n",
      "0.34444444444444444\n",
      "0.3333333333333333\n",
      "0.3333333333333333\n",
      "0.35555555555555557\n",
      "0.43333333333333335\n",
      "0.4444444444444444\n",
      "0.5555555555555556\n",
      "0.24444444444444444\n",
      "0.2777777777777778\n",
      "0.2111111111111111\n",
      "0.4222222222222222\n",
      "0.36666666666666664\n",
      "0.3888888888888889\n",
      "0.34444444444444444\n",
      "0.6111111111111112\n",
      "0.35555555555555557\n",
      "0.32222222222222224\n",
      "0.4666666666666667\n",
      "0.5444444444444444\n",
      "0.37777777777777777\n",
      "0.4\n",
      "0.4111111111111111\n",
      "0.7\n",
      "0.37777777777777777\n",
      "0.34444444444444444\n",
      "0.34444444444444444\n",
      "0.5555555555555556\n",
      "0.25555555555555554\n",
      "0.23333333333333334\n",
      "0.2777777777777778\n",
      "0.5888888888888889\n",
      "---\n",
      "0.3894444444444444\n",
      "0.036867251271790545\n"
     ]
    }
   ],
   "source": [
    "sub_ids = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "# pm/f1_def_results_%s.csv\n",
    "# lda/static_lda_sub_%s.csv\n",
    "# lstm/lstm_sub_%s.csv\n",
    "\n",
    "total_collect = []\n",
    "for sid in sub_ids:\n",
    "    #df1 = pd.read_csv(\"pm/f1_def_results_%s.csv\" % sid)\n",
    "    #df2 = pd.read_csv(\"lda/dynamic_lda_sub_%s.csv\" % sid)\n",
    "    #p,r,f1,a,bacc = calculate_statistics_ensemble(df1, df2)\n",
    "    dfr = pd.read_csv(\"lda/static_lda_sub_%s.csv\" % sid)\n",
    "    p,r,f1,a,bacc = calculate_statistics(dfr)\n",
    "    metric_list = r\n",
    "    p10 = []\n",
    "    p30 = []\n",
    "    p50 = []\n",
    "    p70 = []\n",
    "    p100 = []\n",
    "    for i in range(len(metric_list)):\n",
    "        if i%5 == 0:\n",
    "            p10.append(metric_list[i])\n",
    "        if i%5 == 1:\n",
    "            p30.append(metric_list[i])\n",
    "        if i%5 == 2:\n",
    "            p50.append(metric_list[i])\n",
    "        if i%5 == 3:\n",
    "            p70.append(metric_list[i])\n",
    "        if i%5 == 4:\n",
    "            p100.append(metric_list[i])\n",
    "        \n",
    "    print(statistics.mean(p10))\n",
    "    total_collect.append(statistics.mean(p10))\n",
    "    print(statistics.mean(p30))\n",
    "    total_collect.append(statistics.mean(p30))\n",
    "    print(statistics.mean(p50))\n",
    "    total_collect.append(statistics.mean(p50))\n",
    "    print(statistics.mean(p70))\n",
    "    total_collect.append(statistics.mean(p70))\n",
    "    #print(statistics.mean(p100))\n",
    "    #total_collect.append(statistics.mean(p100))\n",
    "    \n",
    "print(\"---\")\n",
    "print(statistics.mean(total_collect))\n",
    "\n",
    "_, _, margin_of_error= confidence_interval(total_collect)\n",
    "print(margin_of_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.814$\\pm$0.04\n",
      "0.95$\\pm$0.023\n",
      "0.844$\\pm$0.038\n",
      "0.889$\\pm$0.033\n",
      "0.856$\\pm$0.036\n",
      "0.725$\\pm$0.046\n",
      "0.772$\\pm$0.044\n",
      "0.731$\\pm$0.046\n",
      "0.781$\\pm$0.043\n",
      "0.894$\\pm$0.032\n"
     ]
    }
   ],
   "source": [
    "# overall of each subject\n",
    "for subject_id in [1,2,3,4,5,6,7,8,9,10]:\n",
    "    data = pd.read_csv(\"pm/f1_def_results_%s.csv\"%subject_id)\n",
    "    p, r, f1, a, bacc = calculate_statistics(data)\n",
    "    \n",
    "    metric = r\n",
    "    remove_100 = []\n",
    "    for i in range(len(metric)):\n",
    "        if i%5 != 4:\n",
    "            remove_100.append(metric[i])\n",
    "    lower, upper, margin_of_error= confidence_interval(remove_100)\n",
    "    mean = statistics.mean(remove_100)\n",
    "    print(\"%s$\\pm$%s\" % (round(mean, 3), round(margin_of_error,3)))\n",
    "    # print(round(mean, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.336$\\pm$0.031\n",
      "0.346$\\pm$0.031\n",
      "0.353$\\pm$0.031\n",
      "0.523$\\pm$0.033\n",
      "0.709$\\pm$0.03\n"
     ]
    }
   ],
   "source": [
    "## each level of observation averaged over all subjects\n",
    "sub_ids = [1,2,3,4,5,6,7,8,9,10]\n",
    "p10 = []\n",
    "p30 = []\n",
    "p50 = []\n",
    "p70 = []\n",
    "p100 = []\n",
    "# pm/f1_def_results_%s.csv\n",
    "# lda/static_lda_sub_%s.csv\n",
    "# lstm/lstm_sub_%s.csv\n",
    "for sid in sub_ids:\n",
    "    dfr = pd.read_csv(\"lda/static_lda_sub_%s.csv\" % sid)\n",
    "    p,r,f1,a,bacc = calculate_statistics(dfr)\n",
    "    metric_list = p\n",
    "\n",
    "    for i in range(len(metric_list)):\n",
    "        if i%5 == 0:\n",
    "            p10.append(metric_list[i])\n",
    "        if i%5 == 1:\n",
    "            p30.append(metric_list[i])\n",
    "        if i%5 == 2:\n",
    "            p50.append(metric_list[i])\n",
    "        if i%5 == 3:\n",
    "            p70.append(metric_list[i])\n",
    "        if i%5 == 4:\n",
    "            p100.append(metric_list[i])\n",
    "\n",
    "for metric in [p10,p30,p50,p70,p100]:    \n",
    "    lower, upper, margin_of_error= confidence_interval(metric)\n",
    "    mean = statistics.mean(metric)\n",
    "    print(\"%s$\\pm$%s\" % (round(mean, 3), round(margin_of_error,3)))\n",
    "\n",
    "# print(statistics.mean(p10))\n",
    "# print(statistics.mean(p30))\n",
    "# print(statistics.mean(p50))\n",
    "# print(statistics.mean(p70))\n",
    "# print(statistics.mean(p100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dft = pd.read_csv(\"PMGR/sub1/kmeans60_f32_results_1.0.csv\")\n",
    "# dft = pd.read_csv(\"PMGR_new/sub8/kmeans100_f34_sub8.csv\")\n",
    "dft = pd.read_csv(\"lstm/lstm_sub_1.csv\")\n",
    "p,r,f1,a,bacc = calculate_statistics(dft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hypothesis tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The clusters are significantly different.\n",
      "1.18773383024453e-34\n",
      "1.187734e-34\n",
      "0.000e+00\n"
     ]
    }
   ],
   "source": [
    "def aver90cases(file):\n",
    "    total_collect = []\n",
    "    for sid in sub_ids:\n",
    "        dfr = pd.read_csv(file)\n",
    "        p,r,f1,a,bacc = calculate_statistics(dfr)\n",
    "        metric_list = r   ####\n",
    "        p10 = []\n",
    "        p30 = []\n",
    "        p50 = []\n",
    "        p70 = []\n",
    "        p100 = []\n",
    "        for i in range(len(metric_list)):\n",
    "            if i%5 == 0:\n",
    "                p10.append(metric_list[i])\n",
    "            if i%5 == 1:\n",
    "                p30.append(metric_list[i])\n",
    "            if i%5 == 2:\n",
    "                p50.append(metric_list[i])\n",
    "            if i%5 == 3:\n",
    "                p70.append(metric_list[i])\n",
    "            if i%5 == 4:\n",
    "                p100.append(metric_list[i])\n",
    "\n",
    "        total_collect.append(statistics.mean(p10))\n",
    "        total_collect.append(statistics.mean(p30))\n",
    "        total_collect.append(statistics.mean(p50))\n",
    "        total_collect.append(statistics.mean(p70))\n",
    "        # total_collect.append(statistics.mean(p100))\n",
    "    return total_collect\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "sub_ids = [1,2,3,4,5,6,7,8,9,10]\n",
    "for subject_id in sub_ids:\n",
    "    file1 = \"pm/f1_def_results_%s.csv\"%subject_id\n",
    "    #file2 = \"lstm/lstm_sub_%s.csv\"%subject_id\n",
    "    file2 = \"lda/static_lda_sub_%s.csv\"%subject_id\n",
    "    cluster1_data = aver90cases(file1)\n",
    "    cluster2_data = aver90cases(file2)\n",
    "\n",
    "# Perform t-test\n",
    "t_statistic, p_value = stats.ttest_ind(cluster1_data, cluster2_data)\n",
    "\n",
    "# Check significance\n",
    "alpha = 0.05  # Significance level\n",
    "if p_value < alpha:\n",
    "    print(\"The clusters are significantly different.\")\n",
    "else:\n",
    "    print(\"The clusters are not significantly different.\")\n",
    "\n",
    "print(p_value)\n",
    "print(format(p_value, \"e\"))\n",
    "print(format(round(p_value, 3), \".3e\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select N features and M clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29, 1, 2, 34, 32, 28, 22, 34, 23, 28]\n",
      "[70, 10, 150, 50, 90, 160, 80, 100, 100, 170]\n",
      "[0.5766666666666667, 0.5348148148148149, 0.6392592592592593, 0.5214814814814814, 0.5840740740740741, 0.5622222222222222, 0.5307407407407407, 0.5844444444444444, 0.5574074074074074, 0.6037037037037037]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# \"manual\", \"kmeans\"\n",
    "classifier_option = \"kmeans\"\n",
    "selected_num_features = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,\n",
    "    21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47]\n",
    "# diff\n",
    "clusters_num = [10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200]\n",
    "\n",
    "selected_nf = []\n",
    "selected_nc = []\n",
    "max_metric_list = []\n",
    "\n",
    "for sid in [1,2,3,4,5,6,7,8,9,10]:\n",
    "    results_dir = \"PMGR_new/sub%s\"%sid\n",
    "    cwd = os.getcwd()\n",
    "    \n",
    "    max_nf = 0\n",
    "    max_nc = 0\n",
    "    max_metric = 0\n",
    "    \n",
    "    for fn in selected_num_features:\n",
    "        for cl in clusters_num:\n",
    "\n",
    "            csv_file = \"%s%s_f%s_sub%s.csv\" % (classifier_option, str(cl), str(fn), str(sid) )\n",
    "            abs_path_csv_file = os.path.join(cwd, results_dir, csv_file)\n",
    "            if os.path.exists(abs_path_csv_file):\n",
    "                data = pd.read_csv(abs_path_csv_file)\n",
    "                p, r, f1, a, bacc = calculate_statistics(data)\n",
    "                metric = statistics.mean(f1)  #####\n",
    "                if metric > max_metric:\n",
    "                    max_nf = fn\n",
    "                    max_nc = cl\n",
    "                    max_metric = metric\n",
    "                \n",
    "            else:\n",
    "                print(csv_file + \"    Not Exist\")\n",
    "                \n",
    "    selected_nf.append(max_nf)\n",
    "    selected_nc.append(max_nc)\n",
    "    max_metric_list.append(max_metric)\n",
    "\n",
    "\n",
    "print(selected_nf)  ### features\n",
    "print(selected_nc)  ### clusters\n",
    "print(max_metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5694814814814815"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics.mean(max_metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignments calculation times per goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52197745.72592592"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def timePerGoal(rows):\n",
    "    length = rows.shape[0]\n",
    "    g_time = [] \n",
    "    for index, row in rows.iterrows():\n",
    "        stime = row[\"STime\"].split(\"/\")\n",
    "        time_string = stime[0:-1]  # the last one is /\n",
    "        for t in time_string:\n",
    "            g_time.append(int(t))\n",
    "    \n",
    "    return g_time\n",
    "\n",
    "\n",
    "df_pm_t = pd.read_csv(\"pm/f1_def_results_1.csv\")\n",
    "tg_list = timePerGoal(df_pm_t)\n",
    "statistics.mean(tg_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show PRIM parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ema_workbench import load_results\n",
    "\n",
    "\n",
    "delta_list  = []\n",
    "lamb_list  = []\n",
    "phi_list = []\n",
    "theta_list  = []\n",
    "\n",
    "f1_list = []\n",
    "\n",
    "sub_id = 1\n",
    "for sub_id in [1,2,3,4,5,6,7,8,9,10]:\n",
    "    gz = \"prim/f1_100_scenarios_sub%s_.tar.gz\" % sub_id\n",
    "    experiments, outcomes = load_results(gz)\n",
    "\n",
    "    value = outcomes[\"f1\"]\n",
    "    max_metric_id = np.argmax(value, axis=0)\n",
    "    f1 = np.max(value, axis=0)\n",
    "    # because in experiments there is a line of column name\n",
    "    e = experiments.iloc[max_metric_id]\n",
    "\n",
    "    delta_list.append(e[\"delta\"])\n",
    "    lamb_list.append(e[\"lamb\"])\n",
    "    phi_list.append(e[\"phi\"])\n",
    "    theta_list.append(e[\"theta\"])\n",
    "    f1_list.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>delta</th>\n",
       "      <th>lamb</th>\n",
       "      <th>phi</th>\n",
       "      <th>theta</th>\n",
       "      <th>scenario</th>\n",
       "      <th>policy</th>\n",
       "      <th>model</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.093954</td>\n",
       "      <td>3.329053</td>\n",
       "      <td>91</td>\n",
       "      <td>0.618475</td>\n",
       "      <td>31</td>\n",
       "      <td>None</td>\n",
       "      <td>pmgr</td>\n",
       "      <td>0.505185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       delta      lamb  phi     theta  scenario policy model        f1\n",
       "31  0.093954  3.329053   91  0.618475        31   None  pmgr  0.505185"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = \"f1\"\n",
    "\n",
    "gz = \"prim/f1_100_scenarios_sub9_.tar.gz\"\n",
    "experiments, outcomes = load_results(gz)\n",
    "experiments[metric] = outcomes[metric]\n",
    "num = int(len(experiments.index) * 0.01)\n",
    "df = experiments.sort_values(by=[metric], ascending=False).head(num)\n",
    "\n",
    "df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6318518518518519,\n",
       " 0.5455555555555556,\n",
       " 0.6055555555555555,\n",
       " 0.562962962962963,\n",
       " 0.695925925925926,\n",
       " 0.5711111111111111,\n",
       " 0.5248148148148148,\n",
       " 0.6196296296296296,\n",
       " 0.5051851851851852,\n",
       " 0.7725925925925926]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7824170924598155,\n",
       " 0.8340306113890947,\n",
       " 0.9585674041259294,\n",
       " 0.6293604347180899,\n",
       " 0.9123723785617636,\n",
       " 0.8663436685722214,\n",
       " 0.9362941960129322,\n",
       " 0.7788576835572925,\n",
       " 0.6184749109756382,\n",
       " 0.9543071678016238]"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6035185185185186"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics.mean(f1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeWrongProb(wrongProbTuple):\n",
    "    sumProbDiff = []\n",
    "    for item in wrongProbTuple:\n",
    "        predictID = np.argmax(item[1])\n",
    "        predictProb = item[0][0][predictID]\n",
    "        trueID = np.argmax(item[2])\n",
    "        trueProb = item[0][0][trueID]\n",
    "        sumProbDiff.append(predictProb - trueProb)\n",
    "\n",
    "    #print(statistics.mean(sumProbDiff))\n",
    "    print(len(sumProbDiff))\n",
    "    \n",
    "    return 0\n",
    "\n",
    "    \n",
    "    \n",
    "def probDiffWrong(rows):\n",
    "    length = rows.shape[0]\n",
    "\n",
    "    wrongProbDiff = []\n",
    "        \n",
    "    for index, row in rows.iterrows():\n",
    "        \n",
    "        prob = row[\"Prob\"].split(\"/\")\n",
    "        trueID = row[\"Real_Goal\"]\n",
    "        results = row[\"Results\"].split(\"/\")\n",
    "        all_candidates = row[\"Cost\"].split(\"/\")\n",
    "        \n",
    "        total = len(all_candidates)-1   # the last one is /\n",
    "        \n",
    "        bacc = func_bacc(total, results, trueID)\n",
    "        r = func_recall(results, trueID)\n",
    "        \n",
    "        if bacc != 1:\n",
    "            \n",
    "            counter = 0\n",
    "            prob_sum = 0\n",
    "            maxProb = 0\n",
    "            for i in results:\n",
    "                if i:\n",
    "                    if float(prob[int(i)]) > maxProb:\n",
    "                        maxProb = float(prob[int(i)])\n",
    "                    counter += 1\n",
    "                    prob_sum += float(prob[int(i)])\n",
    "            averageProb = prob_sum/counter\n",
    "            trueProb = float(prob[trueID])\n",
    "            # print(averageProb)\n",
    "            # wrongProbDiff.append(averageProb-trueProb)\n",
    "            wrongProbDiff.append(maxProb-trueProb)\n",
    "    \n",
    "    return wrongProbDiff\n",
    "\n",
    "        \n",
    "        \n",
    "# when it's wrong\n",
    "def calculate_wrong_cases(rows):\n",
    "    length = rows.shape[0]\n",
    "    \n",
    "    wrongProbDiff = []\n",
    "    for index, row in rows.iterrows():\n",
    "        \n",
    "        prob = row[\"Prob\"].split(\"/\")\n",
    "        answer = row[\"Real_Goal\"]\n",
    "        results = row[\"Results\"].split(\"/\")\n",
    "        all_candidates = row[\"Cost\"].split(\"/\")\n",
    "        total = len(all_candidates)-1   # the last one is /\n",
    "        \n",
    "        p = func_precision(results, answer)\n",
    "        r = func_recall(results, answer)\n",
    "        f = func_f1(total, results, answer)\n",
    "        a = func_accuracy(total, results, answer)\n",
    "        bacc = func_bacc(total, results, answer)\n",
    "        \n",
    "        if bacc != 1:\n",
    "            maxProb = 0\n",
    "            for i in results:\n",
    "                if i and float(prob[int(i)]) > maxProb:\n",
    "                    maxProb = float(prob[int(i)])\n",
    "                    \n",
    "            trueProb = float(prob[int(answer)])\n",
    "            wrongProbDiff.append(maxProb-trueProb)\n",
    "    \n",
    "    return wrongProbDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove the last 10 points from source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove last 10 datapoints\n",
    "\n",
    "# sort the df to traces for every subject and goals\n",
    "def extract_traces(dfn):\n",
    "    traces = []\n",
    "\n",
    "    Subject = 0\n",
    "    Loc = 0\n",
    "    Iteration = 0\n",
    "    tup = (Subject, Loc, Iteration)\n",
    "\n",
    "    for index, row in dfn.iterrows():\n",
    "        curr_Subject = row[\"Subject\"]\n",
    "        curr_Loc = row[\"Loc\"]\n",
    "        curr_Iteration = row[\"Iteration\"]\n",
    "        curr_tup = (curr_Subject, curr_Loc, curr_Iteration)\n",
    "\n",
    "        if curr_tup != tup:\n",
    "            #print(\"new trace\")\n",
    "            tup = curr_tup\n",
    "\n",
    "            rslt_df = dfn[(dfn['Subject'] == curr_Subject) \n",
    "                      & (dfn['Loc'] == curr_Loc) \n",
    "                      & (dfn['Iteration'] == curr_Iteration)]\n",
    "\n",
    "            rslt_df.reset_index(drop=True, inplace=True)\n",
    "            traces.append(rslt_df)\n",
    "            \n",
    "    return traces\n",
    "\n",
    "\n",
    "input_data_file = \"Corrected_UpperLimbReachingData_AddVelocity&FinalStatic.csv\"\n",
    "output_data_file = \"corrected_final_input_data.csv\"\n",
    "\n",
    "df = pd.read_csv(input_data_file)\n",
    "df_traces = extract_traces(df)\n",
    "\n",
    "df_remove_10_last = pd.DataFrame([])\n",
    "\n",
    "for df_a_trace in df_traces:\n",
    "    df_remove_10_last = pd.concat([df_remove_10_last, df_a_trace.iloc[0:-10,:]], axis=0)\n",
    "    \n",
    "df_remove_10_last.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_remove_10_last.to_csv(output_data_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence Interval: [2.073513677044878, 9.926486322955121]\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as st\n",
    "\n",
    "def confidence_interval(data, confidence=0.95):\n",
    "    n = len(data)\n",
    "    mean = sum(data) / n\n",
    "    std_error = st.sem(data)\n",
    "    margin_of_error = std_error * st.t.ppf((1 + confidence) / 2, n - 1)\n",
    "    lower_bound = mean - margin_of_error\n",
    "    upper_bound = mean + margin_of_error\n",
    "    return lower_bound, upper_bound, margin_of_error\n",
    "\n",
    "# Example usage\n",
    "data = [2, 4, 6, 8, 10]\n",
    "lower, upper, margin_of_error= confidence_interval(data)\n",
    "\n",
    "print(f\"Confidence Interval: [{lower}, {upper}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.926486322955122"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = \n",
    "_, _, margin_of_error= confidence_interval(data)\n",
    "margin_of_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
