{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import math\n",
    "\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_goal_to_binary(goal, length):\n",
    "\n",
    "    results = []\n",
    "    for i in range(length):\n",
    "        if i == goal:\n",
    "            results.append(1)\n",
    "        else:\n",
    "            results.append(0)\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def bacc4binary(pred, answer):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    total = len(pred)\n",
    "    \n",
    "    for i in range(total):\n",
    "        if pred[i] == 1 and answer[i] == 1:\n",
    "            tp += 1\n",
    "        if pred[i] == 0 and answer[i] == 0:\n",
    "            tn += 1\n",
    "        if pred[i] == 1 and answer[i] == 0:\n",
    "            fp += 1\n",
    "        if pred[i] == 0 and answer[i] == 1:\n",
    "            fn += 1\n",
    "\n",
    "    tpr = tp/(tp + fn)\n",
    "    tnr = tn/(tn + fp)\n",
    "    bacc = (tpr + tnr)/2\n",
    "\n",
    "    return bacc\n",
    "\n",
    "def metrics4binary(pred, answer, metric):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    total = len(pred)\n",
    "    \n",
    "    for i in range(total):\n",
    "        if pred[i] == 1 and answer[i] == 1:\n",
    "            tp += 1\n",
    "        if pred[i] == 0 and answer[i] == 0:\n",
    "            tn += 1\n",
    "        if pred[i] == 1 and answer[i] == 0:\n",
    "            fp += 1\n",
    "        if pred[i] == 0 and answer[i] == 1:\n",
    "            fn += 1\n",
    "            \n",
    "    m = 0\n",
    "    if metric == \"p\":\n",
    "        m = tp/(tp + fp)\n",
    "    if metric == \"r\":\n",
    "        m = tp/(tp + fn)\n",
    "    if metric == \"f1\":\n",
    "        m = 2*tp/(2*tp + fp + fn)\n",
    "\n",
    "    return m\n",
    "\n",
    "\n",
    "def select_features(df, labels, features, n_features):\n",
    "    ## create empty lists as placeholders\n",
    "    grouped_features = []\n",
    "    for i in range(n_features):\n",
    "        new = []\n",
    "        grouped_features.append(new)\n",
    "\n",
    "    for i in range(len(features)):\n",
    "        grouped_features[labels[i]].append(features[i])\n",
    "\n",
    "    selected_features = []\n",
    "    for fs in grouped_features:\n",
    "        matrix = df[fs].corr().abs()\n",
    "        max_f_id = matrix.sum(axis=1).argmax()\n",
    "        selected_features.append(fs[max_f_id])\n",
    "        \n",
    "    return selected_features\n",
    "\n",
    "# sort the df to traces for every subject and goals\n",
    "def extract_traces(dfn):\n",
    "    traces = []\n",
    "\n",
    "    Subject = 0\n",
    "    Loc = 0\n",
    "    Iteration = 0\n",
    "    tup = (Subject, Loc, Iteration)\n",
    "\n",
    "    for index, row in dfn.iterrows():\n",
    "        curr_Subject = row[\"Subject\"]\n",
    "        curr_Loc = row[\"Loc\"]\n",
    "        curr_Iteration = row[\"Iteration\"]\n",
    "        curr_tup = (curr_Subject, curr_Loc, curr_Iteration)\n",
    "\n",
    "        if curr_tup != tup:\n",
    "            #print(\"new trace\")\n",
    "            tup = curr_tup\n",
    "\n",
    "            rslt_df = dfn[(dfn['Subject'] == curr_Subject) \n",
    "                      & (dfn['Loc'] == curr_Loc) \n",
    "                      & (dfn['Iteration'] == curr_Iteration)]\n",
    "\n",
    "            rslt_df.reset_index(drop=True, inplace=True)\n",
    "            traces.append(rslt_df)\n",
    "            \n",
    "    return traces\n",
    "\n",
    "\n",
    "def convert2subtraces(traces, goals, subject_id):\n",
    "    # generate classifiers here\n",
    "    \n",
    "    subtraces = []\n",
    "    for goal in goals:\n",
    "        subtraces_goalX = []\n",
    "        for t in traces:\n",
    "            if t[\"Subject\"][0] == subject_id and t[\"Loc\"][0] == goal:                     \n",
    "                subtraces_goalX.append(t)\n",
    "        subtraces.append(subtraces_goalX)\n",
    "    return subtraces\n",
    "\n",
    "\n",
    "def reCreateDir(dirName):\n",
    "    # Check whether the specified path exists or not\n",
    "    isExist = os.path.exists(dirName)\n",
    "    if isExist:\n",
    "        # delete\n",
    "        shutil.rmtree(dirName)\n",
    "    \n",
    "    os.makedirs(dirName)\n",
    "    \n",
    "    \n",
    "def pick_a_point_by_per(trace, header_len, per=None):\n",
    "    if per == None:\n",
    "        feature_values = []\n",
    "        goal = []\n",
    "        for idx in range(len(trace)):\n",
    "            row = trace.iloc[idx]\n",
    "            feature_values.append(row.values[header_len::]) \n",
    "            goal.append(row[\"Loc\"])\n",
    "    else:\n",
    "        idx = math.ceil(len(trace) * per) - 1\n",
    "        row = trace.iloc[idx]\n",
    "        feature_values = row.values[header_len::]\n",
    "        goal = row[\"Loc\"]\n",
    "        \n",
    "    return feature_values, goal\n",
    "\n",
    "\n",
    "    \n",
    "def prepare_data(traces, test_id, percent_list, header_len):\n",
    "    total_num_traces = len(traces)\n",
    "    training_set_x = []\n",
    "    training_set_y = []\n",
    "    test_x = []\n",
    "    test_y = []\n",
    "\n",
    "    # test_id = 5\n",
    "    for i in range(total_num_traces):\n",
    "        \n",
    "        if i != test_id:\n",
    "            x, y = pick_a_point_by_per(traces[i], header_len)  # or remove percent\n",
    "            training_set_x += x\n",
    "            training_set_y += y\n",
    "        else:\n",
    "            for percent in percent_list:\n",
    "                x, y = pick_a_point_by_per(traces[i], header_len, percent)\n",
    "                test_x.append(x)\n",
    "                test_y.append(y)\n",
    "            \n",
    "    return training_set_x, training_set_y, test_x, test_y\n",
    "\n",
    "def random_features(features, select_nums):\n",
    "    random.shuffle(features)\n",
    "    return features[0:select_nums]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/cluster/_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/cluster/_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/cluster/_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/cluster/_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/cluster/_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/cluster/_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/cluster/_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/cluster/_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/cluster/_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/cluster/_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "\n",
    "# parameters\n",
    "sub_ids = [1,2,3,4,5,6,7,8,9,10]\n",
    "features_num_list = [29, 1, 2, 34, 32, 28, 22, 34, 23, 28]\n",
    "num_candidates = 3\n",
    "## the number of irrelevant features : index_headers\n",
    "header_len = 4\n",
    "index_headers = 4\n",
    "obs = [0.1, 0.3, 0.5, 0.7, 1.0]\n",
    "\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "# main script\n",
    "input_data = \"corrected_final_input_data.csv\"\n",
    "df = pd.read_csv(input_data)\n",
    "all_features = df.columns.values.tolist()[index_headers::]\n",
    "df_context = df[all_features]\n",
    "    \n",
    "############################## select features ##############################\n",
    "\n",
    "baccListAllSub = []\n",
    "wrongProbTupleAllSub = []\n",
    "for subject_id in sub_ids:\n",
    "    n_features = features_num_list[subject_id-1]\n",
    "    \n",
    "    corr_matrix = df_context.corr().abs()\n",
    "    hierarchical_cluster = AgglomerativeClustering(n_clusters=n_features, affinity='euclidean', linkage='ward').fit(corr_matrix)\n",
    "    labels = hierarchical_cluster.labels_\n",
    "\n",
    "    selected_features = select_features(df, labels, all_features, n_features)\n",
    "    #selected_features = random_features(all_features, n_features)\n",
    "    output_results = \"lda/dynamic_lda_sub_%s.csv\"%subject_id\n",
    "\n",
    "    df_a_subject = df.loc[df['Subject'] == subject_id].reset_index(drop=True)\n",
    "    if n_features == 1:\n",
    "        reduced = df_a_subject[[\"Subject\", \"Loc\", \"Iteration\"] + selected_features + selected_features]\n",
    "    else:\n",
    "        reduced = df_a_subject[[\"Subject\", \"Loc\", \"Iteration\"] + selected_features]\n",
    "    traces = extract_traces(reduced)\n",
    "    \n",
    "    goals = list(df_a_subject[\"Loc\"].unique())\n",
    "    goals.sort()\n",
    "    subtraces = convert2subtraces(traces, goals, subject_id)\n",
    "    \n",
    "    os.system(\"rm -rf %s\" % output_results)\n",
    "    df_for_output = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    for test_id in range(30):\n",
    "        training_set_x = []\n",
    "        training_set_y = []\n",
    "        test_x = []\n",
    "        test_y = []\n",
    "        for i in range(len(subtraces)):  # how many goals\n",
    "            X, Y, x, y = prepare_data(subtraces[i], test_id, obs, header_len)\n",
    "            training_set_x += X\n",
    "            training_set_y += Y\n",
    "\n",
    "            test_x += x\n",
    "            test_y += y\n",
    "            \n",
    "        clf = LinearDiscriminantAnalysis()\n",
    "        clf.fit(training_set_x, training_set_y)\n",
    "        pred = clf.predict(test_x)\n",
    "        prob = clf.predict_proba(test_x)\n",
    "        \n",
    "        \n",
    "        pred_str = [str(int(p))+\"/\" for p in pred]\n",
    "        real_str = [str(int(ty)) for ty in test_y]\n",
    "        \n",
    "        prob_str = []\n",
    "        for pl in prob:\n",
    "            pstr = \"\"\n",
    "            for pv in pl:\n",
    "                pstr += str(pv)+\"/\"\n",
    "            prob_str.append(pstr)\n",
    "        \n",
    "        new_rows = {'Real_Goal': real_str, 'Cost': prob_str, 'Prob': prob_str, 'Results': pred_str}\n",
    "        #df_for_output = pd.DataFrame(new_rows)\n",
    "        df_for_output = df_for_output.append(pd.DataFrame(new_rows), ignore_index=True)\n",
    "\n",
    "        \n",
    "    df_for_output.to_csv(output_results, index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trained with last 10 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_split10(traces, percent_list, header_len):\n",
    "    total_num_traces = len(traces)\n",
    "    training_set_x = []\n",
    "    training_set_y = []\n",
    "    test_x = []\n",
    "    test_y = []\n",
    "\n",
    "    # test_id = 5\n",
    "    for i in range(total_num_traces):\n",
    "        \n",
    "        x, y = pick_a_point_by_per(traces[i][-10::], header_len)  # or remove percent\n",
    "        training_set_x += x\n",
    "        training_set_y += y\n",
    "\n",
    "        for percent in percent_list:\n",
    "            x, y = pick_a_point_by_per(traces[i][0:-10], header_len, percent)\n",
    "            test_x.append(x)\n",
    "            test_y.append(y)\n",
    "            \n",
    "    return training_set_x, training_set_y, test_x, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/cluster/_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/cluster/_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/cluster/_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/cluster/_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/cluster/_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/cluster/_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/cluster/_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/cluster/_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/cluster/_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/sklearn/cluster/_agglomerative.py:983: FutureWarning: Attribute `affinity` was deprecated in version 1.2 and will be removed in 1.4. Use `metric` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "\n",
    "# parameters\n",
    "sub_ids = [1,2,3,4,5,6,7,8,9,10]\n",
    "# sub_ids = [1]\n",
    "features_num_list = [29, 1, 2, 34, 32, 28, 22, 34, 23, 28]\n",
    "num_candidates = 3\n",
    "## the number of irrelevant features : index_headers\n",
    "header_len = 4\n",
    "index_headers = 4\n",
    "obs = [0.1, 0.3, 0.5, 0.7, 1.0]\n",
    "\n",
    "\n",
    "\n",
    "#############################################################################\n",
    "# main script\n",
    "input_data = \"Corrected_UpperLimbReachingData_AddVelocity&FinalStatic.csv\"\n",
    "df = pd.read_csv(input_data)\n",
    "all_features = df.columns.values.tolist()[index_headers::]\n",
    "df_context = df[all_features]\n",
    "    \n",
    "############################## select features ##############################\n",
    "\n",
    "baccListAllSub = []\n",
    "wrongProbTupleAllSub = []\n",
    "for subject_id in sub_ids:\n",
    "    n_features = features_num_list[subject_id-1]\n",
    "    \n",
    "    corr_matrix = df_context.corr().abs()\n",
    "    hierarchical_cluster = AgglomerativeClustering(n_clusters=n_features, affinity='euclidean', linkage='ward').fit(corr_matrix)\n",
    "    labels = hierarchical_cluster.labels_\n",
    "\n",
    "    #selected_features = select_features(df, labels, all_features, n_features)\n",
    "    selected_features = random_features(all_features, n_features)\n",
    "    output_results = \"lda/static_lda_subr_%s.csv\"%subject_id\n",
    "\n",
    "    df_a_subject = df.loc[df['Subject'] == subject_id].reset_index(drop=True)\n",
    "    if n_features == 1:\n",
    "        reduced = df_a_subject[[\"Subject\", \"Loc\", \"Iteration\"] + selected_features + selected_features]\n",
    "    else:\n",
    "        reduced = df_a_subject[[\"Subject\", \"Loc\", \"Iteration\"] + selected_features]\n",
    "    traces = extract_traces(reduced)\n",
    "    \n",
    "    goals = list(df_a_subject[\"Loc\"].unique())\n",
    "    goals.sort()\n",
    "    subtraces = convert2subtraces(traces, goals, subject_id)\n",
    "    \n",
    "    os.system(\"rm -rf %s\" % output_results)\n",
    "    df_for_output = pd.DataFrame()\n",
    "    \n",
    "    # train model with the last 10 points\n",
    "    training_set_x = []\n",
    "    training_set_y = []\n",
    "    test_x = []\n",
    "    test_y = []\n",
    "    for i in range(len(subtraces)):  # how many goals\n",
    "        X, Y, x, y = prepare_data_split10(subtraces[i], obs, header_len)\n",
    "        training_set_x += X\n",
    "        training_set_y += Y\n",
    "\n",
    "        test_x += x\n",
    "        test_y += y\n",
    "\n",
    "    clf = LinearDiscriminantAnalysis()\n",
    "    clf.fit(training_set_x, training_set_y)\n",
    "    pred = clf.predict(test_x)\n",
    "    prob = clf.predict_proba(test_x)\n",
    "        \n",
    "    pred_str = [str(int(p))+\"/\" for p in pred]\n",
    "    real_str = [str(int(ty)) for ty in test_y]\n",
    "\n",
    "    prob_str = []\n",
    "    for pl in prob:\n",
    "        pstr = \"\"\n",
    "        for pv in pl:\n",
    "            pstr += str(pv)+\"/\"\n",
    "        prob_str.append(pstr)\n",
    "\n",
    "    new_rows = {'Real_Goal': real_str, 'Cost': prob_str, 'Prob': prob_str, 'Results': pred_str}\n",
    "    #df_for_output = pd.DataFrame(new_rows)\n",
    "    df_for_output = df_for_output.append(pd.DataFrame(new_rows), ignore_index=True)\n",
    "\n",
    "        \n",
    "    df_for_output.to_csv(output_results, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
