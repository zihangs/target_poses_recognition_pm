{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_precision(stringList, answer):\n",
    "    goal_count = 0\n",
    "    found = 0\n",
    "    for result in stringList:\n",
    "        if result and int(result) == int(answer):\n",
    "            found = 1\n",
    "        goal_count += 1\n",
    "\n",
    "    return found/(goal_count-1)\n",
    "\n",
    "def func_recall(stringList, answer):\n",
    "    found = 0\n",
    "    for result in stringList:\n",
    "        if result and int(result) == int(answer):\n",
    "            found = 1\n",
    "            break\n",
    "    return found\n",
    "\n",
    "def func_f1(total, stringList, answer):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for result in stringList[0:-1]:\n",
    "        if result and int(result) == int(answer):\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    \n",
    "    fn = 1 - tp\n",
    "    \n",
    "    # total is the number of all goals\n",
    "    tn = total - tp - fp - fn\n",
    "    return 2*tp/(2*tp + fp + fn)\n",
    "\n",
    "def func_accuracy(total, stringList, answer):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for result in stringList[0:-1]:\n",
    "        if result and int(result) == int(answer):\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    \n",
    "    fn = 1 - tp\n",
    "    \n",
    "    # total is the number of all goals\n",
    "    tn = total - tp - fp - fn\n",
    "    return (tp + tn)/(tn + tp + fp + fn)\n",
    "\n",
    "\n",
    "def func_bacc(total, stringList, answer):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for result in stringList[0:-1]:\n",
    "        if result and int(result) == int(answer):\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    \n",
    "    fn = 1 - tp\n",
    "    \n",
    "    # total is the number of all goals\n",
    "    tn = total - tp - fp - fn\n",
    "\n",
    "    tpr = tp/(tp + fn)\n",
    "    tnr = tn/(tn + fp)\n",
    "    bacc = (tpr + tnr)/2\n",
    "\n",
    "    return bacc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# return a list of each statistics for every testing case\n",
    "def calculate_statistics(rows):\n",
    "    length = rows.shape[0]\n",
    "\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1_score = []\n",
    "    accuracy = []\n",
    "    b_accuracy = []\n",
    "        \n",
    "    for index, row in rows.iterrows():\n",
    "        \n",
    "        answer = row[\"Real_Goal\"]\n",
    "        results = row[\"Results\"].split(\"/\")\n",
    "        all_candidates = row[\"Cost\"].split(\"/\")\n",
    "        \n",
    "        total = len(all_candidates)-1   # the last one is /\n",
    "        \n",
    "        p = func_precision(results, answer)\n",
    "        r = func_recall(results, answer)\n",
    "        f = func_f1(total, results, answer)\n",
    "        a = func_accuracy(total, results, answer)\n",
    "        bacc = func_bacc(total, results, answer)\n",
    "        \n",
    "        precision.append(p)\n",
    "        recall.append(r)\n",
    "        f1_score.append(f)\n",
    "        accuracy.append(a)\n",
    "        b_accuracy.append(bacc)\n",
    "    \n",
    "    return precision, recall, f1_score, accuracy, b_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n",
      "0.3888888888888889\n",
      "0.4888888888888889\n",
      "0.5555555555555556\n",
      "0.6888888888888889\n",
      "0.37777777777777777\n",
      "0.34444444444444444\n",
      "0.36666666666666664\n",
      "0.36666666666666664\n",
      "0.6777777777777778\n",
      "0.45555555555555555\n",
      "0.5\n",
      "0.5444444444444444\n",
      "0.7111111111111111\n",
      "0.7111111111111111\n",
      "0.4444444444444444\n",
      "0.4111111111111111\n",
      "0.4222222222222222\n",
      "0.4777777777777778\n",
      "0.5333333333333333\n",
      "0.35555555555555557\n",
      "0.4444444444444444\n",
      "0.3888888888888889\n",
      "0.4888888888888889\n",
      "0.6333333333333333\n",
      "0.3111111111111111\n",
      "0.35555555555555557\n",
      "0.34444444444444444\n",
      "0.37777777777777777\n",
      "0.6333333333333333\n",
      "0.32222222222222224\n",
      "0.43333333333333335\n",
      "0.32222222222222224\n",
      "0.43333333333333335\n",
      "0.5333333333333333\n",
      "0.35555555555555557\n",
      "0.3888888888888889\n",
      "0.4666666666666667\n",
      "0.6\n",
      "0.6777777777777778\n",
      "0.35555555555555557\n",
      "0.3888888888888889\n",
      "0.3888888888888889\n",
      "0.45555555555555555\n",
      "0.6111111111111112\n",
      "0.4111111111111111\n",
      "0.4444444444444444\n",
      "0.5111111111111111\n",
      "0.6222222222222222\n",
      "0.7111111111111111\n"
     ]
    }
   ],
   "source": [
    "sub_ids = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "# prim_results/f1_def_results_%s.csv\n",
    "# lda/dynamic_lda_sub_%s.csv\n",
    "# lstm/lstm_sub_%s.csv\n",
    "for sid in sub_ids:\n",
    "    dfr = pd.read_csv(\"lstm/lstm_sub_%s.csv\" % sid)\n",
    "    p,r,f1,a,bacc = calculate_statistics(dfr)\n",
    "    metric_list = r\n",
    "    p10 = []\n",
    "    p30 = []\n",
    "    p50 = []\n",
    "    p70 = []\n",
    "    p100 = []\n",
    "    for i in range(len(metric_list)):\n",
    "        if i%5 == 0:\n",
    "            p10.append(metric_list[i])\n",
    "        if i%5 == 1:\n",
    "            p30.append(metric_list[i])\n",
    "        if i%5 == 2:\n",
    "            p50.append(metric_list[i])\n",
    "        if i%5 == 3:\n",
    "            p70.append(metric_list[i])\n",
    "        if i%5 == 4:\n",
    "            p100.append(metric_list[i])\n",
    "        \n",
    "    print(statistics.mean(p10))\n",
    "    print(statistics.mean(p30))\n",
    "    print(statistics.mean(p50))\n",
    "    print(statistics.mean(p70))\n",
    "    print(statistics.mean(p100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6314814814814814\n",
      "0.5014814814814815\n",
      "0.6548148148148148\n",
      "0.5418518518518518\n",
      "0.6885185185185185\n",
      "0.5581481481481482\n",
      "0.51\n",
      "0.6059259259259259\n",
      "0.47888888888888886\n",
      "0.7425925925925926\n"
     ]
    }
   ],
   "source": [
    "# overall of each subject\n",
    "for subject_id in [1,2,3,4,5,6,7,8,9,10]:\n",
    "    #output_results: lstm/lstm_subr_%s.csv\n",
    "    data = pd.read_csv(\"prim_results/f1_def_results_%s.csv\"%subject_id, usecols=[0,1,2,3,4])\n",
    "    p, r, f1, a, bacc = calculate_statistics(data)\n",
    "    \n",
    "    metric = f1\n",
    "    print(statistics.mean(metric))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48444444444444446\n",
      "0.56\n",
      "0.6311111111111111\n",
      "0.6511111111111111\n",
      "0.6622222222222223\n"
     ]
    }
   ],
   "source": [
    "## each level of observation averaged over all subjects\n",
    "sub_ids = [1,2,3,4,5,6,7,8,9,10]\n",
    "p10 = []\n",
    "p30 = []\n",
    "p50 = []\n",
    "p70 = []\n",
    "p100 = []\n",
    "# prim_results/f1_def_results_%s.csv\n",
    "# lda/last_lda_sub_%s.csv\n",
    "# lstm/lstm_sub_%s.csv\n",
    "for sid in sub_ids:\n",
    "    dfr = pd.read_csv(\"lda/dynamic_lda_sub_%s.csv\" % sid)\n",
    "    p,r,f1,a,bacc = calculate_statistics(dfr)\n",
    "    metric_list = f1\n",
    "\n",
    "    for i in range(len(metric_list)):\n",
    "        if i%5 == 0:\n",
    "            p10.append(metric_list[i])\n",
    "        if i%5 == 1:\n",
    "            p30.append(metric_list[i])\n",
    "        if i%5 == 2:\n",
    "            p50.append(metric_list[i])\n",
    "        if i%5 == 3:\n",
    "            p70.append(metric_list[i])\n",
    "        if i%5 == 4:\n",
    "            p100.append(metric_list[i])\n",
    "        \n",
    "print(statistics.mean(p10))\n",
    "print(statistics.mean(p30))\n",
    "print(statistics.mean(p50))\n",
    "print(statistics.mean(p70))\n",
    "print(statistics.mean(p100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dft = pd.read_csv(\"PMGR/sub1/kmeans60_f32_results_1.0.csv\")\n",
    "# dft = pd.read_csv(\"PMGR_new/sub8/kmeans100_f34_sub8.csv\")\n",
    "dft = pd.read_csv(\"lstm/lstm_sub_1.csv\")\n",
    "p,r,f1,a,bacc = calculate_statistics(dft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "450"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The clusters are significantly different.\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Data points from cluster 1 and cluster 2\n",
    "cluster1_data = [1, 2, 3, 4, 5]\n",
    "cluster2_data = [6, 7, 8, 9, 10]\n",
    "\n",
    "# Perform t-test\n",
    "t_statistic, p_value = stats.ttest_ind(cluster1_data, cluster2_data)\n",
    "\n",
    "# Check significance\n",
    "alpha = 0.05  # Significance level\n",
    "if p_value < alpha:\n",
    "    print(\"The clusters are significantly different.\")\n",
    "else:\n",
    "    print(\"The clusters are not significantly different.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001052825793366539"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select N features and M clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29, 32, 34, 34, 32, 28, 22, 34, 23, 28]\n",
      "[70, 10, 120, 50, 90, 160, 80, 100, 100, 170]\n",
      "[0.5766666666666667, 0.5011111111111111, 0.6259259259259259, 0.5214814814814814, 0.5840740740740741, 0.5622222222222222, 0.5307407407407407, 0.5844444444444444, 0.5574074074074074, 0.6037037037037037]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# \"manual\", \"kmeans\"\n",
    "classifier_option = \"kmeans\"\n",
    "selected_num_features = [21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40]\n",
    "# diff\n",
    "clusters_num = [10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200]\n",
    "\n",
    "selected_nf = []\n",
    "selected_nc = []\n",
    "max_metric_list = []\n",
    "\n",
    "for sid in [1,2,3,4,5,6,7,8,9,10]:\n",
    "    results_dir = \"PMGR_new/sub%s\"%sid\n",
    "    cwd = os.getcwd()\n",
    "    \n",
    "    max_nf = 0\n",
    "    max_nc = 0\n",
    "    max_metric = 0\n",
    "    \n",
    "    for fn in selected_num_features:\n",
    "        for cl in clusters_num:\n",
    "\n",
    "            csv_file = \"%s%s_f%s_sub%s.csv\" % (classifier_option, str(cl), str(fn), str(sid) )\n",
    "            abs_path_csv_file = os.path.join(cwd, results_dir, csv_file)\n",
    "            if os.path.exists(abs_path_csv_file):\n",
    "                data = pd.read_csv(abs_path_csv_file)\n",
    "                p, r, f1, a, bacc = calculate_statistics(data)\n",
    "                metric = statistics.mean(f1)  #####\n",
    "                if metric > max_metric:\n",
    "                    max_nf = fn\n",
    "                    max_nc = cl\n",
    "                    max_metric = metric\n",
    "                \n",
    "            else:\n",
    "                print(csv_file + \"    Not Exist\")\n",
    "                \n",
    "    selected_nf.append(max_nf)\n",
    "    selected_nc.append(max_nc)\n",
    "    max_metric_list.append(max_metric)\n",
    "\n",
    "\n",
    "print(selected_nf)  ### features\n",
    "print(selected_nc)  ### clusters\n",
    "print(max_metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5647777777777777"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistics.mean(max_metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove the last 10 points from source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove last 10 datapoints\n",
    "\n",
    "# sort the df to traces for every subject and goals\n",
    "def extract_traces(dfn):\n",
    "    traces = []\n",
    "\n",
    "    Subject = 0\n",
    "    Loc = 0\n",
    "    Iteration = 0\n",
    "    tup = (Subject, Loc, Iteration)\n",
    "\n",
    "    for index, row in dfn.iterrows():\n",
    "        curr_Subject = row[\"Subject\"]\n",
    "        curr_Loc = row[\"Loc\"]\n",
    "        curr_Iteration = row[\"Iteration\"]\n",
    "        curr_tup = (curr_Subject, curr_Loc, curr_Iteration)\n",
    "\n",
    "        if curr_tup != tup:\n",
    "            #print(\"new trace\")\n",
    "            tup = curr_tup\n",
    "\n",
    "            rslt_df = dfn[(dfn['Subject'] == curr_Subject) \n",
    "                      & (dfn['Loc'] == curr_Loc) \n",
    "                      & (dfn['Iteration'] == curr_Iteration)]\n",
    "\n",
    "            rslt_df.reset_index(drop=True, inplace=True)\n",
    "            traces.append(rslt_df)\n",
    "            \n",
    "    return traces\n",
    "\n",
    "\n",
    "input_data_file = \"Corrected_UpperLimbReachingData_AddVelocity&FinalStatic.csv\"\n",
    "output_data_file = \"corrected_final_input_data.csv\"\n",
    "\n",
    "df = pd.read_csv(input_data_file)\n",
    "df_traces = extract_traces(df)\n",
    "\n",
    "df_remove_10_last = pd.DataFrame([])\n",
    "\n",
    "for df_a_trace in df_traces:\n",
    "    df_remove_10_last = pd.concat([df_remove_10_last, df_a_trace.iloc[0:-10,:]], axis=0)\n",
    "    \n",
    "df_remove_10_last.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_remove_10_last.to_csv(output_data_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
